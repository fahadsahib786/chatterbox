diff --git a/src/chatterbox/vc.py b/src/chatterbox/vc.py
--- a/src/chatterbox/vc.py
+++ b/src/chatterbox/vc.py
@@ class ChatterboxVC:
@@ __init__(self, s3gen, device, ref_dict=None, use_mixed_precision=True, compile_models=True, optimize_quality=True):
-        self.sr = S3GEN_SR
-        self.device = device
-        self.use_mixed_precision = use_mixed_precision and device != "cpu"
-        self.compile_models = compile_models and device != "cpu"
-        self.optimize_quality = optimize_quality
-
-        # Set autocast dtype for mixed precision
-        self.autocast_dtype = torch.float16 if self.use_mixed_precision else torch.float32
+        self.sr = S3GEN_SR
+        self.device = device
+        self.use_mixed_precision = use_mixed_precision and device != "cpu"
+        self.compile_models = compile_models and device != "cpu"
+        self.optimize_quality = optimize_quality
+
+        # Validate device and available features before applying optimizations.
+        # If a CUDA device was requested but no CUDA runtime is available, gracefully fall back to CPU.
+        if "cuda" in self.device and not torch.cuda.is_available():
+            logger.warning(f"Requested CUDA device {self.device} but CUDA is unavailable. Falling back to CPU.")
+            self.device = "cpu"
+        # Similarly, fall back if an MPS device was requested and is not available.
+        if "mps" in self.device and hasattr(torch.backends, "mps") and not torch.backends.mps.is_available():
+            logger.warning("MPS device requested but not available; falling back to CPU.")
+            self.device = "cpu"
+        # Disable model compilation if torch.compile is not present (e.g. on older PyTorch versions).
+        if self.compile_models and not hasattr(torch, "compile"):
+            logger.warning("torch.compile not available in this PyTorch build; disabling model compilation.")
+            self.compile_models = False
+        # Mixed precision requires a CUDA or MPS device; disable it on CPU.
+        if self.use_mixed_precision and self.device == "cpu":
+            logger.warning("Mixed precision requires a CUDA or MPS device; disabling mixed precision.")
+            self.use_mixed_precision = False
+
+        # Set autocast dtype for mixed precision (float16) or full precision (float32)
+        self.autocast_dtype = torch.float16 if self.use_mixed_precision else torch.float32
@@ def generate(self, audio, target_voice_path=None, optimize_quality=None):
-        if target_voice_path:
-            self.set_target_voice(target_voice_path)
-        else:
-            assert self.ref_dict is not None, "Please `prepare_conditionals` first or specify `target_voice_path`"
+        if target_voice_path:
+            self.set_target_voice(target_voice_path)
+        else:
+            # Provide a clearer error message when no voice reference has been set.
+            if self.ref_dict is None:
+                raise RuntimeError(
+                    "No target voice reference provided. Call `set_target_voice` with a reference sample "
+                    "or specify `target_voice_path` before calling generate()."
+                )
@@ def generate(self, audio, target_voice_path=None, optimize_quality=None):
-                     if isinstance(audio, str):
-                         audio_16, _ = librosa.load(audio, sr=S3_SR)
-                         # Apply gentle pre-processing
-                         if optimize_quality:
-                             audio_16 = self._preprocess_audio(audio_16)
-                     else:
-                         audio_16 = audio
-                         
-                     audio_16 = torch.from_numpy(audio_16).float().to(self.device)[None, ]
+                     if isinstance(audio, str):
+                         audio_16, _ = librosa.load(audio, sr=S3_SR)
+                         if optimize_quality:
+                             audio_16 = self._preprocess_audio(audio_16)
+                     elif isinstance(audio, np.ndarray):
+                         audio_16 = audio
+                     elif torch.is_tensor(audio):
+                         audio_16 = audio.detach().cpu().numpy()
+                     else:
+                         raise TypeError("Unsupported audio type; expected file path, numpy array, or torch tensor.")
+                     
+                     audio_16 = torch.from_numpy(np.asarray(audio_16, dtype=np.float32)).to(self.device)[None, ]
diff --git a/src/chatterbox/tts.py b/src/chatterbox/tts.py
--- a/src/chatterbox/tts.py
+++ b/src/chatterbox/tts.py
@@ class ChatterboxTTS:
@@ __init__(self, t3: T3, s3gen: S3Gen, ve: VoiceEncoder, tokenizer: EnTokenizer, device: str, conds: Conditionals = None, use_mixed_precision: bool = True, compile_models: bool = True, optimize_memory: bool = True, ):
-        self.sr = S3GEN_SR  # sample rate of synthesized audio
-        self.device = device
-        self.use_mixed_precision = use_mixed_precision and device != "cpu"
-        self.compile_models = compile_models and device != "cpu"
-        self.optimize_memory = optimize_memory
+        self.sr = S3GEN_SR  # sample rate of synthesized audio
+        self.device = device
+        self.use_mixed_precision = use_mixed_precision and device != "cpu"
+        self.compile_models = compile_models and device != "cpu"
+        self.optimize_memory = optimize_memory
+
+        # Validate device and available features.
+        if "cuda" in self.device and not torch.cuda.is_available():
+            logger.warning(f"Requested CUDA device {self.device} but CUDA is unavailable. Falling back to CPU.")
+            self.device = "cpu"
+        if "mps" in self.device and hasattr(torch.backends, "mps") and not torch.backends.mps.is_available():
+            logger.warning("MPS device requested but not available; falling back to CPU.")
+            self.device = "cpu"
+        if self.compile_models and not hasattr(torch, "compile"):
+            logger.warning("torch.compile not available in this PyTorch build; disabling model compilation.")
+            self.compile_models = False
+        if self.use_mixed_precision and self.device == "cpu":
+            logger.warning("Mixed precision requires a CUDA or MPS device; disabling mixed precision.")
+            self.use_mixed_precision = False
@@ __init__(self, ...):
-        # Set autocast dtype for mixed precision
-        # Prefer bfloat16 on Ampere+ (e.g., A40) for speed and stability; fallback to float16 otherwise
-        if self.use_mixed_precision:
-            dtype = torch.float16
-            try:
-                if torch.cuda.is_available() and hasattr(torch.cuda, "is_bf16_supported") and torch.cuda.is_bf16_supported():
-                    dtype = torch.bfloat16
-            except Exception:
-                # Safe fallback if CUDA is not available or function is missing
-                pass
-            self.autocast_dtype = dtype
-        else:
-            self.autocast_dtype = torch.float32
+        # Set autocast dtype for mixed precision. Prefer bfloat16 on supported GPUs; otherwise use float16.
+        # On CPU fall back to float32.
+        if self.use_mixed_precision:
+            dtype = torch.float16
+            try:
+                if torch.cuda.is_available() and hasattr(torch.cuda, "is_bf16_supported") and torch.cuda.is_bf16_supported():
+                    dtype = torch.bfloat16
+            except Exception:
+                pass
+            self.autocast_dtype = dtype
+        else:
+            self.autocast_dtype = torch.float32
